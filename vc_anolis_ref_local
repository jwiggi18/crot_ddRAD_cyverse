#creating crot vcf file with acar refrence genome locally

#note: if workign in iCloud, make sure your files are downloaded from the cloud or the shell can't 'see' them

#create new folder system for vcf analysis
#cd to the crots ddRAD file
#laptop filepath
cd ~/iCloud/com~apple~CloudDocs/2.research/crots/ddRAD

#desktop filepath
#cd ~/Library/Mobile\ Documents/com~apple~CloudDocs/2.research/crots/ddRAD

#make a directory called 'vcf' with subdirectories 'data' and 'results'
mkdir vcf
mkdir vcf/data vcf/results

#move the file containing individual lizard .gz files and move it to 'vcf/data'
mv processed3_f_m vcf/data

#check that folder moved
cd vcf/data
ls

#couldnt get recursive to work
#recursively unzip the fq.gz files
#gunzip -r "$processed3_f_m"

#unzip files
cd processed3_f_m/cc_f
gunzip *.gz
cd ../cc_m
gunzip *.gz

#navigate to vcf directory
cd ..

#make empty directory for reference genome
mkdir -p data/ref_genome

#locate genome
# search: https://www.ncbi.nlm.nih.gov/assembly/?term=Anolis+carolinensis
# click: AnoCar2.0
# on the right click 'FTP directory for GenBank Assembly'
#get reference genome
curl -L -o data/ref_genome/acar.fasta.gz ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/090/745/GCA_000090745.2_AnoCar2.0/GCA_000090745.2_AnoCar2.0_genomic.fna.gz 

#unzip
gunzip data/ref_genome/acar.fasta.gz

#make directories for file types
mkdir -p results/sam results/bam results/bcf results/vcf

# BWA http://bio-bwa.sourceforge.net/bwa.shtml
#create an index for the reference genome
bwa index data/ref_genome/acar.fasta

#variant call on on a single individual to test the code

#use bwa to produce a sam file
bwa mem data/ref_genome/acar.fasta data/processed3_f_m/cc_f/CC_F_BG119.fq > results/sam/CC_F_BG119.aligned.sam 

#Samtools https://genome.sph.umich.edu/wiki/SAM
#download here: http://www.htslib.org/download/
#convert binary SAM file to BAM file with input SAM (-S) and output BAM (-b)
samtools view -S -b results/sam/CC_F_BG119.aligned.sam > results/bam/CC_F_BG119.aligned.bam

#sort the BAM file according to the reference genome (-o location of output)
samtools sort -o results/bam/CC_F_BG119.aligned.sorted.bam results/bam/CC_F_BG119.aligned.bam 

#look at flags (what are our issues?)
samtools flagstat results/bam/CC_F_BG119.aligned.sorted.bam

#bcftools
#count read depth, generate a bcf output (-O b), where and what the output (-o), reference genome location (-f), input
bcftools mpileup -O b -o results/bcf/CC_F_BG119_raw.bcf -f data/ref_genome/acar.fasta results/bam/CC_F_BG119.aligned.sorted.bam 

#detect variants, allow for multialleleic and rare variants (-m), output variant sites only (-v), where and what the output (-o)
bcftools call --ploidy 2 -m -v -o results/vcf/CC_F_BG119_variants.vcf results/bcf/CC_F_BG119_raw.bcf 

#filter & report variants in variaint calling format (vcf) using samtools' vcfutils.pl
vcfutils.pl varFilter results/vcf/CC_F_BG119_variants.vcf  > results/vcf/CC_F_BG119_final_variants.vcf

#check to see how many variants were removed
wc -l results/vcf/*

#index the BAM file for visualization
samtools index results/bam/CC_F_BG119.aligned.sorted.bam



#run the script to loop through all individuals
bash variant_calling.sh

#for some reason that is beyond my comprehension but I am doing to assume isn't stupid (even thought it seems really stupid) we have to compress all the stupid .vcf files we just created to vcf.gz then index them with tabix to make bcftools happy so we can merge the stupid vcf files. I'm not cranky, you're cranky. 
#how to loop this?
bgzip -c file.vcf > file.vcf.gz
#or 
bcftools view file.vcf -Oz -o file.vcf.gz

#index
tabix -p vcf file.vcf.gz
#or
bcftools index file.vcf.gz

#merge the resulting vcf files
bcftools 